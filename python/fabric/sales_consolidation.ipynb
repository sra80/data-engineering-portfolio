{"cells":[{"cell_type":"markdown","source":["# Sales Orders Processing (NAV → EXT)\n","\n","This notebook consolidates **current** and **archived** NAV sales data into clean, partitioned EXT tables for downstream BI consumption.  \n","It performs the following steps:\n","- Reads **sales header** and **sales line** data from NAV (current + archive).  \n","- Merges rows so that **current records take priority**, and archived records are included only when `archive_reason = 3`.  \n","- Flags each row with `is_archived` (Boolean).  \n","- Filters to include only orders from the past **two years** (by `order_date`).  \n","- Maintains lightweight **meta tracker tables** to track missing or inactive IDs.  \n","- Writes partitioned outputs into the `ext.sales_header` and `ext.sales_line` tables, ready for reporting and analysis.  \n","\n","This process ensures a consistent, deduplicated view of sales orders across both current and historical sources."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7f7525b0-0e97-45f5-a540-f6fadb858210"},{"cell_type":"markdown","source":["# 0 — Config & constants"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"65964bbd-ec49-41e7-8a5d-999271c460a1"},{"cell_type":"code","source":["from pyspark.sql import functions as F, types as T, Window\n","import datetime\n","\n","# Source schema (all four inputs live here)\n","NAV_DB = \"nav\"\n","\n","# Output schema\n","EXT_SCHEMA = \"ext\"\n","\n","# Separate trackers (recommended)\n","TRACKER_HEADER_TBL = \"meta.sales_header\"\n","TRACKER_LINE_TBL   = \"meta.sales_line\"\n","\n","# Primary key\n","ID_COL = \"id\"\n","\n","# For pretty ordering in outputs\n","ORDER_ANCHORS = [\"id\", \"company_id\"]"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"ad70c9c2-6571-4cc8-9ed2-474c4983b770","normalized_state":"finished","queued_time":"2025-08-27T10:48:47.1356487Z","session_start_time":null,"execution_start_time":"2025-08-27T10:48:54.1675782Z","execution_finish_time":"2025-08-27T10:48:54.5837464Z","parent_msg_id":"57cffeb4-7e78-4f94-a92a-198dc73b2394"},"text/plain":"StatementMeta(, ad70c9c2-6571-4cc8-9ed2-474c4983b770, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"75b1e247-4a14-4009-8bfc-511a0778e878"},{"cell_type":"markdown","source":["# 1 — Core utilities (I/O, schema align, whitelists, partition helpers)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c6151f87-351b-433e-b8ae-0482dd0985cf"},{"cell_type":"code","source":["def table_exists(fully_qualified: str) -> bool:\n","    try:\n","        spark.read.table(fully_qualified).limit(1).collect()\n","        return True\n","    except Exception:\n","        return False\n","\n","def safe_read_table(tbl: str, schema: T.StructType = None):\n","    if table_exists(tbl):\n","        return spark.read.table(tbl)\n","    return spark.createDataFrame([], schema or T.StructType([T.StructField(ID_COL, T.StringType(), True)]))\n","\n","def normalize_id_str(df, id_col=ID_COL):\n","    return df.withColumn(id_col, F.col(id_col).cast(T.StringType()))\n","\n","def reorder_cols_id_company_first(df):\n","    cols = df.columns\n","    front = [c for c in [\"id\", \"company_id\"] if c in cols]\n","    rest  = [c for c in cols if c not in front]\n","    return df.select(*front, *rest)\n","\n","# ------- Schema alignment -------\n","def _add_missing_from(df_target, df_ref):\n","    ref_types = {f.name: f.dataType for f in df_ref.schema.fields}\n","    for name, dtype in ref_types.items():\n","        if name not in df_target.columns:\n","            df_target = df_target.withColumn(name, F.lit(None).cast(dtype))\n","    return df_target\n","\n","def _align_pair(left, right):\n","    left2  = _add_missing_from(left,  right)\n","    right2 = _add_missing_from(right, left)\n","    ordered = list(dict.fromkeys(left.columns + [c for c in right.columns if c not in left.columns]))\n","    return left2.select(*ordered), right2.select(*ordered)\n","\n","# ------- Column whitelisting (safe if missing) -------\n","def _lit_null(dtype):\n","    return F.lit(None).cast(dtype if dtype is not None else T.StringType())\n","\n","def project_whitelist(df, keep_cols, *, type_hints=None):\n","    \"\"\"\n","    Return df with exactly keep_cols (in this order),\n","    creating missing columns as NULL with dtype from type_hints (else STRING).\n","    \"\"\"\n","    type_hints = type_hints or {}\n","    existing = set(df.columns)\n","    out_cols = []\n","    for c in keep_cols:\n","        if c in existing:\n","            out_cols.append(F.col(c))\n","        else:\n","            out_cols.append(_lit_null(type_hints.get(c)).alias(c))\n","    return df.select(*out_cols)\n","\n","# ------- Partition helpers -------\n","def yymm_as_int(col):\n","    \"\"\"Return yyMM as INT (e.g., 2025-08-01 -> 2508).\"\"\"\n","    return F.date_format(col, \"yyMM\").cast(T.IntegerType())"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"ad70c9c2-6571-4cc8-9ed2-474c4983b770","normalized_state":"finished","queued_time":"2025-08-27T10:48:47.23853Z","session_start_time":null,"execution_start_time":"2025-08-27T10:48:54.5858595Z","execution_finish_time":"2025-08-27T10:48:54.9038549Z","parent_msg_id":"8ac9a81c-785d-4cb4-8f76-ef4cccdf71e2"},"text/plain":"StatementMeta(, ad70c9c2-6571-4cc8-9ed2-474c4983b770, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1153639f-3767-4cf4-ac98-500117a2752c"},{"cell_type":"markdown","source":["# 2 — NAV loader + preferred rows"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a9607333-8233-4d39-82ee-b2999f873ae7"},{"cell_type":"code","source":["# === Cell 2 : NAV loader + preferred rows (Boolean is_archived only) ===\n","from pyspark.sql import functions as F, types as T, Window\n","\n","def _load_nav_pair(name: str):\n","    \"\"\"\n","    name ∈ {'sales_header','sales_line'}\n","    Reads: nav.<name> and nav.<name>_archive\n","    Returns aligned (df_arc, df_cur) with ids as STRING.\n","    \"\"\"\n","    cur_tbl = f\"{NAV_DB}.{name}\"\n","    arc_tbl = f\"{NAV_DB}.{name}_archive\"\n","\n","    cur_exists = table_exists(cur_tbl)\n","    arc_exists = table_exists(arc_tbl)\n","    if not cur_exists and not arc_exists:\n","        raise ValueError(f\"No sources found for '{name}' — expected {cur_tbl} and/or {arc_tbl}\")\n","\n","    df_cur = safe_read_table(cur_tbl)\n","    df_arc = safe_read_table(arc_tbl, schema=df_cur.schema) if arc_exists else spark.createDataFrame([], df_cur.schema)\n","\n","    # align both directions; normalize id\n","    df_arc, df_cur = _align_pair(df_arc, df_cur)\n","    df_arc = normalize_id_str(df_arc)\n","    df_cur = normalize_id_str(df_cur)\n","    return df_arc, df_cur\n","\n","def build_preferred_rows_header(df_arc, df_cur, id_col: str):\n","    \"\"\"\n","    HEADERS:\n","      - Prefer CURRENT rows if they exist (is_archived=False)\n","      - Else take ARCHIVE rows only when archive_reason = 3 (is_archived=True)\n","      - Drop ids that only have archive with reason != 3\n","    Ensures archive_reason exists on both sides (NULL on current).\n","    \"\"\"\n","    # ensure archive_reason exists on both sides with consistent type\n","    if \"archive_reason\" not in df_cur.columns:\n","        df_cur = df_cur.withColumn(\"archive_reason\", F.lit(None).cast(T.IntegerType()))\n","    else:\n","        df_cur = df_cur.withColumn(\"archive_reason\", F.col(\"archive_reason\").cast(T.IntegerType()))\n","    if \"archive_reason\" not in df_arc.columns:\n","        df_arc = df_arc.withColumn(\"archive_reason\", F.lit(None).cast(T.IntegerType()))\n","    else:\n","        df_arc = df_arc.withColumn(\"archive_reason\", F.col(\"archive_reason\").cast(T.IntegerType()))\n","\n","    # tag boolean is_archived\n","    df_arc = df_arc.withColumn(\"is_archived\", F.lit(True))\n","    df_cur = df_cur.withColumn(\"is_archived\", F.lit(False))\n","\n","    combined = df_arc.unionByName(df_cur, allowMissingColumns=True)\n","\n","    # rank: current wins (2), archived&reason3 next (1), archived other (0 -> excluded)\n","    rank = (\n","        F.when(F.col(\"is_archived\") == False, F.lit(2))\n","         .when((F.col(\"is_archived\") == True) & (F.col(\"archive_reason\") == 3), F.lit(1))\n","         .otherwise(F.lit(0))\n","    )\n","    w = Window.partitionBy(id_col).orderBy(rank.desc())\n","\n","    preferred = (combined\n","                 .withColumn(\"_rank\", rank)\n","                 .withColumn(\"_rn\", F.row_number().over(w))\n","                 .filter((F.col(\"_rn\") == 1) & (F.col(\"_rank\") > 0))\n","                 .drop(\"_rn\", \"_rank\"))\n","    return preferred\n","\n","def build_preferred_rows_header(df_arc, df_cur, id_col: str):\n","    \"\"\"\n","    HEADERS:\n","      - Prefer CURRENT rows if they exist (is_archived=False)\n","      - Else take ARCHIVE rows only when archive_reason = 3 (is_archived=True)\n","      - Drop ids that only have archive with reason != 3\n","    Ensures archive_reason exists on both sides (NULL on current).\n","    \"\"\"\n","    # ensure archive_reason exists and is INT on both sides\n","    if \"archive_reason\" not in df_cur.columns:\n","        df_cur = df_cur.withColumn(\"archive_reason\", F.lit(None).cast(T.IntegerType()))\n","    else:\n","        df_cur = df_cur.withColumn(\"archive_reason\", F.col(\"archive_reason\").cast(T.IntegerType()))\n","    if \"archive_reason\" not in df_arc.columns:\n","        df_arc = df_arc.withColumn(\"archive_reason\", F.lit(None).cast(T.IntegerType()))\n","    else:\n","        df_arc = df_arc.withColumn(\"archive_reason\", F.col(\"archive_reason\").cast(T.IntegerType()))\n","\n","    # tag boolean is_archived\n","    df_arc = df_arc.withColumn(\"is_archived\", F.lit(True))\n","    df_cur = df_cur.withColumn(\"is_archived\", F.lit(False))\n","\n","    combined = df_arc.unionByName(df_cur, allowMissingColumns=True)\n","\n","    # rank: current wins (2), archived&reason3 next (1), archived other (0 -> excluded)\n","    rank = (\n","        F.when(F.col(\"is_archived\") == False, F.lit(2))\n","         .when((F.col(\"is_archived\") == True) & (F.col(\"archive_reason\") == 3), F.lit(1))\n","         .otherwise(F.lit(0))\n","    )\n","    w = Window.partitionBy(id_col).orderBy(rank.desc())\n","\n","    preferred = (combined\n","                 .withColumn(\"_rank\", rank)\n","                 .withColumn(\"_rn\", F.row_number().over(w))\n","                 .filter((F.col(\"_rn\") == 1) & (F.col(\"_rank\") > 0))\n","                 .drop(\"_rn\", \"_rank\"))\n","    return preferred\n","\n","def build_preferred_rows_line(df_arc, df_cur, id_col: str):\n","    \"\"\"\n","    LINES:\n","      - Prefer CURRENT rows if they exist (is_archived=False)\n","      - Else take ARCHIVE rows (is_archived=True)\n","      - Lines don’t have archive_reason; filtering by reason=3 is done later\n","        via the header lookup in the EXT writer.\n","    \"\"\"\n","    # tag boolean is_archived\n","    df_arc = df_arc.withColumn(\"is_archived\", F.lit(True))\n","    df_cur = df_cur.withColumn(\"is_archived\", F.lit(False))\n","\n","    combined = df_arc.unionByName(df_cur, allowMissingColumns=True)\n","\n","    # rank: current wins (2), archive next (1)\n","    rank = F.when(F.col(\"is_archived\") == False, F.lit(2)).otherwise(F.lit(1))\n","    w = Window.partitionBy(id_col).orderBy(rank.desc())\n","\n","    preferred = (combined\n","                 .withColumn(\"_rank\", rank)\n","                 .withColumn(\"_rn\", F.row_number().over(w))\n","                 .filter(F.col(\"_rn\") == 1)\n","                 .drop(\"_rn\", \"_rank\"))\n","\n","    return preferred\n","\n","def process_pair(name: str, tracker_tbl: str):\n","    \"\"\"\n","    Returns:\n","      preferred : unified rows with Boolean is_archived\n","      tracker   : tracker after upsert for the seen IDs\n","    Uses header/line-specific logic to avoid referencing archive-only columns on lines.\n","    \"\"\"\n","    df_arc, df_cur = _load_nav_pair(name)\n","    if name == \"sales_header\":\n","        preferred = build_preferred_rows_header(df_arc=df_arc, df_cur=df_cur, id_col=ID_COL)\n","    else:\n","        preferred = build_preferred_rows_line(df_arc=df_arc, df_cur=df_cur, id_col=ID_COL)\n","\n","    tracker = upsert_tracker(tracker_tbl, preferred.select(ID_COL, \"is_archived\"))\n","    return preferred, tracker"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"ad70c9c2-6571-4cc8-9ed2-474c4983b770","normalized_state":"finished","queued_time":"2025-08-27T10:48:47.4582015Z","session_start_time":null,"execution_start_time":"2025-08-27T10:48:54.9062246Z","execution_finish_time":"2025-08-27T10:48:55.271217Z","parent_msg_id":"47ec2369-e6c3-4af3-928d-8cacdb09ca5c"},"text/plain":"StatementMeta(, ad70c9c2-6571-4cc8-9ed2-474c4983b770, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f2ed1d32-6045-467f-92c8-74011d4561d6"},{"cell_type":"markdown","source":["# 3 — Trackers (Boolean is_archived, first‑run safe)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5bd935c8-38e5-4c47-bf7f-107f34de099a"},{"cell_type":"code","source":["TRACKER_SCHEMA = T.StructType([\n","    T.StructField(ID_COL, T.StringType(), True),\n","    T.StructField(\"is_archived\", T.BooleanType(), True),        # True = archived, False = current\n","    T.StructField(\"missing_since_ts\", T.TimestampType(), True), # NULL when seen this run\n","    T.StructField(\"deleted_confirmed\", T.BooleanType(), True),\n","    T.StructField(\"updated_at\", T.TimestampType(), True),\n","])\n","\n","def _read_or_create_tracker(tracker_tbl: str):\n","    if table_exists(tracker_tbl):\n","        df = spark.read.table(tracker_tbl)\n","        # Add missing columns & cast to canonical schema\n","        have = set(df.columns)\n","        for f in TRACKER_SCHEMA.fields:\n","            if f.name not in have:\n","                df = df.withColumn(f.name, F.lit(None).cast(f.dataType))\n","        for f in TRACKER_SCHEMA.fields:\n","            df = df.withColumn(f.name, F.col(f.name).cast(f.dataType))\n","        return df.select(*[f.name for f in TRACKER_SCHEMA.fields])\n","    empty = spark.createDataFrame([], schema=TRACKER_SCHEMA)\n","    (empty.write\n","          .format(\"delta\")\n","          .mode(\"overwrite\")\n","          .option(\"overwriteSchema\", \"true\")\n","          .saveAsTable(tracker_tbl))\n","    return spark.read.table(tracker_tbl)\n","\n","def upsert_tracker(tracker_tbl: str, seen_df):\n","    \"\"\"\n","    Input MUST include: id (STRING), is_archived (Boolean).\n","    Overwrite-based tracker:\n","      - seen → missing_since_ts = NULL; update is_archived & updated_at\n","      - not seen → set/retain missing_since_ts\n","    \"\"\"\n","    now = F.current_timestamp()\n","\n","    seen_latest = (seen_df\n","                   .withColumn(ID_COL, F.col(ID_COL).cast(T.StringType()))\n","                   .withColumn(\"is_archived\", F.col(\"is_archived\").cast(T.BooleanType()))\n","                   .select(ID_COL, \"is_archived\")\n","                   .dropDuplicates([ID_COL]))\n","\n","    existing = _read_or_create_tracker(tracker_tbl)\n","\n","    seen_ids = seen_latest.select(ID_COL).dropDuplicates([ID_COL])\n","    missing  = existing.join(seen_ids, on=ID_COL, how=\"left_anti\")\n","\n","    seen_upd = (seen_latest\n","                .withColumn(\"missing_since_ts\", F.lit(None).cast(T.TimestampType()))\n","                .withColumn(\"deleted_confirmed\", F.lit(False))\n","                .withColumn(\"updated_at\", now))\n","\n","    missing_upd = (missing\n","                   .withColumn(\"missing_since_ts\",\n","                               F.when(F.col(\"missing_since_ts\").isNull(), now).otherwise(F.col(\"missing_since_ts\")))\n","                   .withColumn(\"updated_at\", now))\n","\n","    new_tracker = (missing_upd\n","                   .unionByName(seen_upd, allowMissingColumns=True)\n","                   .select(*[f.name for f in TRACKER_SCHEMA.fields]))\n","\n","    (new_tracker.write\n","               .format(\"delta\")\n","               .mode(\"overwrite\")\n","               .option(\"overwriteSchema\", \"true\")\n","               .saveAsTable(tracker_tbl))\n","    return spark.read.table(tracker_tbl)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"ad70c9c2-6571-4cc8-9ed2-474c4983b770","normalized_state":"finished","queued_time":"2025-08-27T10:48:47.4604604Z","session_start_time":null,"execution_start_time":"2025-08-27T10:48:55.273519Z","execution_finish_time":"2025-08-27T10:48:55.6216343Z","parent_msg_id":"8bcf75be-347c-498b-b938-148fb5c3d937"},"text/plain":"StatementMeta(, ad70c9c2-6571-4cc8-9ed2-474c4983b770, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0d3dfb53-9df0-41b9-9968-2f40d17d4b88"},{"cell_type":"markdown","source":["# 4 — Partitioning & EXT writer (clean line join; no duplicate cols)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"90587a46-d1d4-4345-9ad7-f18291c1c503"},{"cell_type":"code","source":["def add_partition_header(df_header):\n","    \"\"\"partition_by = yyMM when archived else 0 (INT).\"\"\"\n","    return df_header.withColumn(\n","        \"partition_by\",\n","        F.when(F.col(\"is_archived\") == True, yymm_as_int(F.col(\"order_date\"))).otherwise(F.lit(0))\n","    )\n","\n","def add_partition_line(df_line, header_df, *, line_header_key=\"header_id\", header_id_col=\"id\"):\n","    \"\"\"\n","    Derive line.partition_by from header via join and return ONLY:\n","      - all original line columns\n","      - derived partition_by (INT)\n","    Header columns are not retained to avoid duplicate names.\n","    \"\"\"\n","    h = (header_df\n","         .select(\n","             F.col(header_id_col).alias(\"_header_key\"),\n","             F.col(\"order_date\").alias(\"_h_order_date\"),\n","             F.col(\"is_archived\").alias(\"_h_is_archived\"),\n","             F.col(\"archive_reason\").alias(\"_h_archive_reason\")\n","         ))\n","    j = (df_line.alias(\"l\")\n","         .join(h.alias(\"h\"), F.col(f\"l.{line_header_key}\") == F.col(\"_header_key\"), \"inner\")\n","         .withColumn(\n","             \"partition_by\",\n","             F.when(F.col(\"_h_is_archived\") == True, yymm_as_int(F.col(\"_h_order_date\"))).otherwise(F.lit(0))\n","         )\n","         .drop(\"_header_key\", \"_h_order_date\", \"_h_is_archived\", \"_h_archive_reason\"))\n","    # ensure no accidental is_archived on line\n","    if \"is_archived\" in j.columns:\n","        j = j.drop(\"is_archived\")\n","    return j\n","\n","def build_ext_output(\n","    preferred_df,\n","    tracker_df,\n","    ext_out_tbl: str,\n","    *,\n","    header_lookup_df=None,          # required for LINE\n","    line_header_key=\"header_id\",\n","    header_id_col=\"id\"\n","):\n","    # Present vs missing (flags only)\n","    present_df = (preferred_df\n","                  .withColumn(\"in_limbo\", F.lit(False))\n","                  .withColumn(\"deleted_confirmed\", F.lit(False))\n","                  .withColumn(\"limbo_started_ts\", F.lit(None).cast(T.TimestampType())))\n","\n","    missing_ids = (tracker_df\n","                   .filter(F.col(\"missing_since_ts\").isNotNull())\n","                   .select(\n","                       F.col(ID_COL),\n","                       F.col(\"missing_since_ts\").alias(\"limbo_started_ts\"),\n","                       F.col(\"deleted_confirmed\"))\n","                   .withColumn(\"in_limbo\", F.when(F.col(\"deleted_confirmed\") == False, True).otherwise(False)))\n","\n","    if missing_ids.rdd.isEmpty():\n","        final_df = present_df\n","    else:\n","        business_cols = [c for c in preferred_df.columns]\n","\n","        # Build a skeleton with all business columns present (NULLs with preferred dtypes)\n","        skeleton = missing_ids\n","        # map preferred dtypes for faithful casting\n","        pref_types = {f.name: f.dataType for f in preferred_df.schema.fields}\n","        for c in business_cols:\n","            if c not in skeleton.columns:\n","                skeleton = skeleton.withColumn(c, F.lit(None).cast(pref_types.get(c, T.StringType())))\n","\n","        skeleton = skeleton.select(*business_cols, \"in_limbo\", \"deleted_confirmed\", \"limbo_started_ts\")\n","\n","        present_df = present_df.select(*business_cols, \"in_limbo\", \"deleted_confirmed\", \"limbo_started_ts\")\n","        final_df   = present_df.unionByName(skeleton, allowMissingColumns=True)\n","\n","    # Partition + filter\n","    if \"order_date\" in final_df.columns:\n","        # HEADER path\n","        final_df = add_partition_header(final_df).filter(\n","            ((F.col(\"is_archived\") == True) & (F.col(\"archive_reason\") == 3)) |\n","            (F.col(\"is_archived\") == False)\n","        )\n","    else:\n","        # LINE path\n","        if header_lookup_df is None:\n","            final_df = add_partition_header(final_df).filter(\n","                ((F.col(\"is_archived\") == True) & (F.col(\"archive_reason\") == 3)) |\n","                (F.col(\"is_archived\") == False)\n","                )\n","        else:\n","            final_df = add_partition_line(final_df, header_lookup_df,\n","                                          line_header_key=line_header_key, header_id_col=header_id_col)\n","            h2 = header_lookup_df.select(\n","            F.col(header_id_col).alias(\"_hid\"),\n","            F.col(\"archive_reason\").alias(\"_h_archive_reason\"),\n","            F.col(\"is_archived\").alias(\"_h_is_archived\")\n","        )\n","        final_df = (final_df.alias(\"l\")\n","                    .join(h2.alias(\"h2\"), F.col(f\"l.{line_header_key}\") == F.col(\"_hid\"), \"inner\")\n","                    .filter(\n","                        ((F.col(\"_h_is_archived\") == True) & (F.col(\"_h_archive_reason\") == 3)) |\n","                        (F.col(\"_h_is_archived\") == False)\n","                    )\n","                    .drop(\"_hid\",\"_h_archive_reason\",\"_h_is_archived\"))\n","\n","    # Order columns: id, company_id first; keep partition_by\n","    final_df = reorder_cols_id_company_first(final_df)\n","\n","    # Persist\n","    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {EXT_SCHEMA}\")\n","    final_df.createOrReplaceTempView(\"_final_ext\")\n","\n","    if not table_exists(ext_out_tbl):\n","        (final_df\n","          .repartition(\"partition_by\")\n","          .write.format(\"delta\").mode(\"overwrite\")\n","          .option(\"overwriteSchema\",\"true\")\n","          .partitionBy(\"partition_by\")\n","          .saveAsTable(ext_out_tbl))\n","    else:\n","        spark.sql(f\"\"\"\n","            MERGE INTO {ext_out_tbl} AS t\n","            USING _final_ext AS s\n","            ON  t.{ID_COL} = s.{ID_COL}\n","            AND t.partition_by = s.partition_by\n","            WHEN MATCHED THEN UPDATE SET *\n","            WHEN NOT MATCHED THEN INSERT *\n","        \"\"\")\n","    return spark.read.table(ext_out_tbl)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"ad70c9c2-6571-4cc8-9ed2-474c4983b770","normalized_state":"finished","queued_time":"2025-08-27T10:48:47.5603826Z","session_start_time":null,"execution_start_time":"2025-08-27T10:48:55.6238268Z","execution_finish_time":"2025-08-27T10:48:55.9698882Z","parent_msg_id":"c0716153-b74a-454d-8774-359423e6e0a4"},"text/plain":"StatementMeta(, ad70c9c2-6571-4cc8-9ed2-474c4983b770, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"98bcdaa9-d62d-4dec-b3db-e23907926b05"},{"cell_type":"markdown","source":["# 5 — Column whitelists & type hints"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"add4f15b-0a9f-4bed-b38b-5391853b1383"},{"cell_type":"code","source":["# Business columns you want to keep (examples — replace with your own)\n","HEADER_WHITELIST = [\n","    \"id\", \"company_id\", \"document_type\", \"no_\", \"doc__no__occurrence\", \"sell_to_customer_no_\", \"order_date\", \"version_no_\", \n","    \"ship_to_country_region_code\", \"channel_code\", \"media_code\", \"payment_method_code\", \"currency_factor\", \"inbound_integration_code\", \n","    \"origin_datetime\", \"created_datetime\", \"order_created_datetime\", \"sales_order_status\", \"status\", \"on_hold\", \"external_document_no_\", \n","    \"subscription_no_\"\n","]\n","\n","LINE_WHITELIST = [\n","    \"id\", \"company_id\", \"header_id\", \"line_no_\", \"location_code\", \"no_\", \"quantity\", \"delivery_service\", \"promotion_discount_amount\", \n","    \"line_discount_amount\", \"amount_including_vat\", \"amount\", \"quantity_shipped\", \"quantity_invoiced\", \"dimension_set_id\", \"type\", \n","    \"outstanding_quantity\", \"bom_item_no_\"\n","]\n","\n","# Optional dtype hints for missing columns\n","HEADER_TYPE_HINTS = {\n","    \"order_date\": T.TimestampType(),\n","    \"posting_date\": T.TimestampType(),\n","    \"archive_reason\": T.IntegerType(),\n","}\n","LINE_TYPE_HINTS = {\n","    \"quantity\": T.DecimalType(18, 6),\n","    \"unit_price\": T.DecimalType(18, 6),\n","    \"line_amount\": T.DecimalType(18, 2),\n","    \"shipment_date\": T.TimestampType(),\n","}"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"ad70c9c2-6571-4cc8-9ed2-474c4983b770","normalized_state":"finished","queued_time":"2025-08-27T10:48:47.6503362Z","session_start_time":null,"execution_start_time":"2025-08-27T10:48:55.9717789Z","execution_finish_time":"2025-08-27T10:48:56.2922262Z","parent_msg_id":"a60534cc-e2ee-4564-b513-97df18a8de89"},"text/plain":"StatementMeta(, ad70c9c2-6571-4cc8-9ed2-474c4983b770, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"16aaccf7-910a-4c49-b89a-1f17c6e26656"},{"cell_type":"markdown","source":["# 6 — Orchestration (cutoff, whitelists, write EXT)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"26907dd7-a2d0-49eb-9725-2470a77d3390"},{"cell_type":"code","source":["# 0) Cutoff: Jan 1 two years ago from \"today\"\n","today = datetime.date.today()\n","cutoff = datetime.date(today.year - 2, 1, 1)   # e.g. 2023-01-01\n","\n","# 1) Build preferred + trackers (from full preferred to avoid false \"missing\")\n","preferred_header, tracker_header = process_pair(\"sales_header\", TRACKER_HEADER_TBL)\n","preferred_line,   tracker_line   = process_pair(\"sales_line\",   TRACKER_LINE_TBL)\n","\n","# 2) Apply cutoff to HEADER (order_date is datetime)\n","preferred_header_cut = preferred_header.filter(F.col(\"order_date\") >= F.lit(cutoff))\n","\n","# 3) Project to whitelisted columns (+ technical required)\n","#    - Keep is_archived (Boolean), archive_reason (INT), order_date (Timestamp) for partition/filter\n","header_keep = list(dict.fromkeys(\n","    HEADER_WHITELIST + [\"is_archived\", \"partition_by\", \"archive_reason\"]       # partition_by will be created later; harmless to include here\n","))\n","preferred_header_cut = project_whitelist(preferred_header_cut, header_keep, type_hints=HEADER_TYPE_HINTS)\n","\n","line_keep = list(dict.fromkeys(\n","    LINE_WHITELIST + [\"partition_by\", \"header_id\"]           # partition_by created later\n","))\n","preferred_line = project_whitelist(preferred_line, line_keep, type_hints=LINE_TYPE_HINTS)\n","\n","# 4) Header lookup for line (derive partition & filter from header) — includes cutoff\n","header_lookup = preferred_header_cut.select(\"id\", \"order_date\", \"is_archived\", \"archive_reason\")\n","\n","# 5) Write EXT tables (partitioned by INT partition_by, filtered to archive_reason=3)\n","results_header = build_ext_output(\n","    preferred_df = preferred_header_cut,\n","    tracker_df   = tracker_header,\n","    ext_out_tbl  = f\"{EXT_SCHEMA}.sales_header\"\n",")\n","results_line = build_ext_output(\n","    preferred_df      = preferred_line,\n","    tracker_df        = tracker_line,\n","    ext_out_tbl       = f\"{EXT_SCHEMA}.sales_line\",\n","    header_lookup_df  = header_lookup,\n","    line_header_key   = \"header_id\",\n","    header_id_col     = \"id\"\n",")\n","\n","print(\"Done ✅\")\n","print(\"ext.sales_header rows:\", results_header.count())\n","print(\"ext.sales_line rows  :\", results_line.count())\n","# display(results_header.limit(10))\n","# display(results_line.limit(10))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"ad70c9c2-6571-4cc8-9ed2-474c4983b770","normalized_state":"finished","queued_time":"2025-08-27T10:48:47.7383771Z","session_start_time":null,"execution_start_time":"2025-08-27T10:48:56.2944118Z","execution_finish_time":"2025-08-27T10:56:30.5088616Z","parent_msg_id":"83a4c5ee-08ab-46ae-b2e0-121a9bb7aba7"},"text/plain":"StatementMeta(, ad70c9c2-6571-4cc8-9ed2-474c4983b770, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Done ✅\next.sales_header rows: 5941265\next.sales_line rows  : 13662745\n"]}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"advisor":{"adviceMetadata":"{\"artifactId\":\"17c7fbfa-3b10-4f16-a58d-c540bd4b6c8e\",\"activityId\":\"ad70c9c2-6571-4cc8-9ed2-474c4983b770\",\"applicationId\":\"application_1756290745645_0001\",\"jobGroupId\":\"9\",\"advices\":{\"warn\":2,\"error\":1}}"}},"id":"c172734d-fd26-4b59-b617-e58d15237ff1"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"89b2c552-3bb8-470e-adf7-eaa4a5e58166"}],"default_lakehouse":"89b2c552-3bb8-470e-adf7-eaa4a5e58166","default_lakehouse_name":"Silver","default_lakehouse_workspace_id":"5b0d98a7-495c-47b2-9e4a-29b6e447a5d6"}}},"nbformat":4,"nbformat_minor":5}